{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax, expit\n",
    "from bin import plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d653db59071d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"../figures/pred_coding\"\n",
    "PUB = True\n",
    "if PUB:\n",
    "    w = 1.5\n",
    "    h = 1\n",
    "else:\n",
    "    w = 3\n",
    "    h = 2\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa191063d0599d",
   "metadata": {},
   "source": [
    "The model is extremely simple, and is defined as three orthogonal feature dimmensions, each with three discrete values, and 4 \"concepts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58772746cb7f1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (2, 3)\n",
    "conceptual_layer = np.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4219a5a8477b0a",
   "metadata": {},
   "source": [
    "If connectivity is fully diagnostic, then each individual color is fully diagnostic of each shape. THe connectivity matrices for each concept are showm with the third and fourth being \"distractor\" concepts unrelated to the task. _feature dimensions are rows, values are cols_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c05e8ba8de33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define concept pieces with numpy slices\n",
    "# DIAGNOSTIC Connectivity\n",
    "concepts = (np.s_[:2, 0], np.s_[:2, 1], np.s_[0, 2])\n",
    "reward = np.array([1, 1, 1, 1])\n",
    "connectivity = np.zeros((input_size) + (3,))\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.set_size_inches((3 * w, h))\n",
    "for j, c in enumerate(concepts):\n",
    "    connectivity[c + (j,)] = 1\n",
    "    axs[j].imshow(connectivity[..., j])\n",
    "    axs[j].grid(True,  color='black', linewidth=2.0, which=\"major\")\n",
    "    axs[j].set_xticks(np.arange(-.5, 3, 1), minor=False)\n",
    "    axs[j].set_yticks(np.arange(-.5, 2, 1), minor=False)\n",
    "    axs[j].set_xticklabels([])\n",
    "    axs[j].set_yticklabels([])\n",
    "fig.savefig(os.path.join(out_path, \"concept_true.svg\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4dee2eec6aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptFeatureModel:\n",
    "\n",
    "    def __init__(self, features=3, feature_dims=2, concept_dims=3, connectivity=connectivity,):\n",
    "        self.percept_size = (features, feature_dims)\n",
    "        self.concept_evd = np.zeros(concept_dims)\n",
    "        self.percept_evd = np.zeros(self.percept_size)\n",
    "        self.connectivity = connectivity\n",
    "        self.n_concepts = concept_dims\n",
    "        self.system_noise_sigma_ = .0\n",
    "        self.measurement_noise_sigma_ = .0\n",
    "\n",
    "    def predict(self, concept_probs: np.ndarray):\n",
    "        # exp = connectivity.sum(axis=1).T # 3, 4\n",
    "        feat_mod = self.connectivity.reshape((-1, self.n_concepts)).T\n",
    "        predicted = concept_probs @ feat_mod # n feat dims\n",
    "        return predicted\n",
    "\n",
    "    def forward(self, percept):\n",
    "        measure_noise = np.random.normal(size=self.percept_size, scale=self.measurement_noise_sigma_)\n",
    "        self.percept_evd = self.percept_evd + .1 * (percept - self.percept_evd)\n",
    "        # prediction from top down\n",
    "        pred = self.predict(self.concept_evd)\n",
    "        # compute residual\n",
    "        res = self.percept_evd.reshape((1, -1)) - pred\n",
    "        # send residual\n",
    "        self.concept_evd += .1 * (res @ self.connectivity.reshape((-1, self.n_concepts))).squeeze()\n",
    "        return res, self.concept_evd\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> tuple[np.ndarray[float]]:\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efba5dea9f64a99",
   "metadata": {},
   "source": [
    "If we present the full complement of stimuli that defines a specific concept, the model quickly converges to the correct concept (top) and the gain remains low (bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_simple_predictive(P, iter=75):\n",
    "    res_act = []\n",
    "    top_act = []\n",
    "    cfm = ConceptFeatureModel(2, 3, 3, connectivity)\n",
    "    top_act.append(0)\n",
    "    for i in range(50):\n",
    "        res, c_evd = cfm(p)\n",
    "        res_act.append(res)\n",
    "        top_act.append(np.abs(c_evd).sum())\n",
    "    res_act = np.concatenate(res_act)\n",
    "    res_pat = np.mean(res_act, axis=0)\n",
    "    total_act = np.sum(np.abs(res_act), axis=1)\n",
    "    return res_pat, total_act"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a77efb251b13d8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430181a4a787d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all percepts\n",
    "percepts = {}\n",
    "\n",
    "# simple stimulus (like achromatic unassociated familiar shape)\n",
    "p = np.array([[0.0, 0.0, 1.0],\n",
    "              [0.0, 0.0, 0.0]])\n",
    "percepts[\"simple\"] = p\n",
    "\n",
    "# congruent stimulus (like color associated shape)\n",
    "p = np.array([[1.0, 0.0, 0.0],\n",
    "              [1.0, 0.0, 0.0]])\n",
    "percepts[\"congruent\"] = p\n",
    "\n",
    "# partial congruent stimulus (like grayscale color associated shape)\n",
    "p = np.array([[1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0]])\n",
    "percepts[\"partial_1\"] = p\n",
    "\n",
    "# partial congruent stimulus (like grayscale color associated shape)\n",
    "p = np.array([[0.0, 0.0, 0.0],\n",
    "              [1.0, 0.0, 0.0]])\n",
    "percepts[\"partial_2\"] = p\n",
    "\n",
    "# incongruent stimulus (like mis-colored color associated shape)\n",
    "p = np.array([[1.0, 0.0, 0.0],\n",
    "              [0.0, 1.0, 0.0]])\n",
    "percepts[\"incongruent\"] = p\n",
    "\n",
    "# show stimuli\n",
    "fig, ax = plt.subplots(1, len(percepts))\n",
    "fig.set_size_inches((5 * w, h))\n",
    "for i, k in enumerate(percepts.keys()):\n",
    "    ax[i].imshow(percepts[k], vmin=0, vmax=1)\n",
    "    ax[i].set_title(k)\n",
    "    ax[i].grid(True,  color='black', linewidth=2.0, which=\"major\")\n",
    "    ax[i].set_xticks(np.arange(-.5, 3, 1), minor=False)\n",
    "    ax[i].set_yticks(np.arange(-.5, 2, 1), minor=False)\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "\n",
    "fig.savefig(os.path.join(out_path, \"tested_stimuli.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# don't need both partial because both features are symmetric. \n",
    "to_plot = [\"simple\", \"congruent\", \"partial_1\", \"incongruent\"]\n",
    "data = []\n",
    "# run models \n",
    "for d in to_plot:\n",
    "    p = percepts[d]\n",
    "    pat, mag = run_simple_predictive(p)\n",
    "    data.append(mag)\n",
    "# create joint activation data for plotting \n",
    "data = np.stack(data)[..., None]\n",
    "mag_fig, mag_ax = plt.subplots()\n",
    "mag_fig.set_size_inches((w, h))\n",
    "mag_fig = plotter.create_save_line_plot(mag_ax, mag_fig, \"congruent_con_probs\", data, out_dir=out_path, set_size=(w, h))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3860f2d2fe7106c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above shows expected magnitude of activation over time in the lower \"perceptual\" layer. Now we ask can we cross decoded two associated features from the perceptual layer. We plot the pattern of activity over the perceptual space averaged over time. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe3920dbbda8ff1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Are the two directions of partially congruent cross decodable? \n",
    "# partial congruent stimulus (like grayscale color associated shape)\n",
    "p = np.array([[1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0]])\n",
    "prt1_pat, prt1_mag = run_simple_predictive(p)\n",
    "\n",
    "p = np.array([[0.0, 0.0, 0.0],\n",
    "              [1.0, 0.0, 0.0]])\n",
    "prt2_pat, prt2_mag = run_simple_predictive(p)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches((2*w, h))\n",
    "\n",
    "plot_loc = [prt1_pat, prt2_pat]\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(plot_loc[i].reshape((2, 3)), vmin=-1, vmax=1)\n",
    "    ax.grid(True,  color='black', linewidth=2.0, which=\"major\")\n",
    "    ax.set_xticks(np.arange(-.5, 3, 1), minor=False)\n",
    "    ax.set_yticks(np.arange(-.5, 2, 1), minor=False)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fig.savefig(os.path.join(out_path, \"percept_pattern_partial.svg\"))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1219c3b0758e30c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The concept layer is trivially cross decodable. There are other related top down frameworks that would show similar results, such as feature gain. However, there is no purely feedforward framework, regardless of normalization, that would explain both the differences in activation and the lack of cross decoding in low level areas. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b76797c6c7bb766b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "49ba4a11e2eb4bab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
